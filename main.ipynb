{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision scikit-learn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上一版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "TRAIN_DATA_PATHS = [f\"Datasets/trainingdata{i}\" for i in range(11)]  \n",
    "TEST_DATA_PATH = \"Datasets/testingdata\"\n",
    "\n",
    "for path in TRAIN_DATA_PATHS + [TEST_DATA_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"資料路徑 {path} 不存在！\")\n",
    "\n",
    "IMG_SIZE = 224  \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0), ratio=(0.8, 1.2)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.2),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_paths, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for class_id, data_path in enumerate(data_paths):\n",
    "            for filename in os.listdir(data_path):\n",
    "                if filename.endswith('.jpg'):\n",
    "                    img_path = os.path.join(data_path, filename)\n",
    "                    # assume \"0_xxxx.jpg\" => label=0\n",
    "                    label = int(filename.split('_')[0])\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train_dataset = CustomImageDataset(\n",
    "    data_paths=TRAIN_DATA_PATHS,\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "\n",
    "\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.transform = transform\n",
    "        for filename in os.listdir(data_path):\n",
    "            if filename.endswith('.jpg'):\n",
    "                img_path = os.path.join(data_path, filename)\n",
    "                self.image_paths.append(img_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "test_dataset = TestImageDataset(TEST_DATA_PATH, transform=val_test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"總訓練資料數量: {len(train_dataset)}\")\n",
    "print(f\"訓練集數量: {len(train_indices)}\")\n",
    "print(f\"驗證集數量: {len(val_indices)}\")\n",
    "print(f\"測試集數量: {len(test_dataset)}\")\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
    "                               kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64,  3, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ResidualBlock(num_classes=11).to(device)\n",
    "    print(model)\n",
    "\n",
    "    dummy_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "    output = model(dummy_input)\n",
    "    print(f\"輸出維度: {output.shape}\")\n",
    "\n",
    "    def count_parameters(m):\n",
    "        return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"模型參數量: {count_parameters(model):,}\")\n",
    "\n",
    "NUM_EPOCHS = 300       \n",
    "LEARNING_RATE = 0.0005\n",
    "PATIENCE = 20          \n",
    "MODEL_SAVE_PATH = \"best_model.pth\"\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for images, labels in train_loader_tqdm:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    val_loader_tqdm = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_tqdm:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_loss = running_loss / total\n",
    "    val_acc = 100.0 * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device):\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-5)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"--> 最佳模型已保存 (Val Acc: {best_val_acc:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"--> 早停於 Epoch {epoch+1}, 最佳 Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, best_val_acc, best_epoch\n",
    "\n",
    "def plot_accuracies(train_accuracies, val_accuracies):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Train and Validation Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig('accuracy_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "# 主程式\n",
    "if __name__ == \"__main__\":\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, best_val_acc, best_epoch = train_model(\n",
    "        model, train_loader, val_loader, device\n",
    "    )\n",
    "    plot_accuracies(train_accuracies, val_accuracies)\n",
    "    print(f\"訓練完成！最佳 Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "\n",
    "def get_tta_variants(tensor_image):\n",
    "    tta_images = []\n",
    "    tta_images.append(tensor_image)\n",
    "    flipped = TF.hflip(tensor_image)\n",
    "    tta_images.append(flipped)\n",
    "\n",
    "    return tta_images\n",
    "\n",
    "def inference_tta(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    indices = []\n",
    "\n",
    "    test_loader_tqdm = tqdm(test_loader, desc=\"Inferencing (TTA)\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_images, img_paths in test_loader_tqdm:\n",
    "            batch_size = batch_images.size(0)\n",
    "\n",
    "            batch_preds = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                single_img = batch_images[i].to(device)\n",
    "                tta_imgs = get_tta_variants(single_img)\n",
    "\n",
    "                all_logits = []\n",
    "                for tta_img in tta_imgs:\n",
    "                    tta_img_batch = tta_img.unsqueeze(0)\n",
    "                    outputs = model(tta_img_batch)\n",
    "                    all_logits.append(outputs)\n",
    "\n",
    "                avg_logits = torch.mean(torch.stack(all_logits, dim=0), dim=0)\n",
    "                _, pred = torch.max(avg_logits, dim=1)\n",
    "                batch_preds.append(pred.item())\n",
    "\n",
    "            for img_path, pred in zip(img_paths, batch_preds):\n",
    "                index = int(os.path.basename(img_path).split('.')[0])\n",
    "                indices.append(index)\n",
    "                predictions.append(pred)\n",
    "\n",
    "            test_loader_tqdm.set_postfix(num_processed=len(indices))\n",
    "\n",
    "    return indices, predictions\n",
    "\n",
    "def save_results(indices, predictions, output_file=\"SampleSubmission.csv\"):\n",
    "    results = list(zip(indices, predictions))\n",
    "    results.sort(key=lambda x: x[0])  \n",
    "    df = pd.DataFrame(results, columns=[\"Index\", \"Label\"])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"推理結果已保存到 {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新版(ResNet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入所需的Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chihhung/.conda/envs/test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "類別分佈: [994, 429, 1500, 986, 848, 1325, 440, 280, 855, 1500, 709]\n",
      "總訓練資料數量: 9866\n",
      "訓練集數量: 7892\n",
      "驗證集數量: 1974\n",
      "測試集數量: 6777\n"
     ]
    }
   ],
   "source": [
    "# 隨機種子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 定義資料路徑\n",
    "TRAIN_DATA_PATHS = [f\"Datasets/trainingdata{i}\" for i in range(11)]\n",
    "TEST_DATA_PATH = \"Datasets/testingdata\"\n",
    "\n",
    "for path in TRAIN_DATA_PATHS + [TEST_DATA_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"資料路徑 {path} 不存在！\")\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "# 定義資料增強\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0), ratio=(0.8, 1.2)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.2),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# 自定義 Dataset \n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_paths, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for class_id, data_path in enumerate(data_paths):\n",
    "            for filename in os.listdir(data_path):\n",
    "                if filename.endswith('.jpg'):\n",
    "                    img_path = os.path.join(data_path, filename)\n",
    "                    label = int(filename.split('_')[0])\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 訓練資料\n",
    "train_dataset = CustomImageDataset(data_paths=TRAIN_DATA_PATHS, transform=train_transforms)\n",
    "\n",
    "# 分割訓練集和驗證集(80% 訓練, 20% 驗證)\n",
    "indices = list(range(len(train_dataset)))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# 創建驗證集的 Sampler\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# 創建訓練和驗證的 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "\n",
    "\n",
    "# 檢查類別分佈\n",
    "class_counts = [0] * 11\n",
    "for _, label in train_dataset:\n",
    "    class_counts[label] += 1\n",
    "print(\"類別分佈:\", class_counts)\n",
    "\n",
    "\n",
    "# 載入測試資料\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.transform = transform\n",
    "        for filename in os.listdir(data_path):\n",
    "            if filename.endswith('.jpg'):\n",
    "                img_path = os.path.join(data_path, filename)\n",
    "                self.image_paths.append(img_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "test_dataset = TestImageDataset(TEST_DATA_PATH, transform=val_test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"總訓練資料數量: {len(train_dataset)}\")\n",
    "print(f\"訓練集數量: {len(train_indices)}\")\n",
    "print(f\"驗證集數量: {len(val_indices)}\")\n",
    "print(f\"測試集數量: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "檢查數據格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50 架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(F.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(F.relu(self.fc1(self.max_pool(x))))\n",
    "        return torch.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_attention(x)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "# Bottleneck 殘差塊\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4  \n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropblock_prob=0.0):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        mid_channels = out_channels // self.expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
    "        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.cbam = CBAM(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.dropblock_prob = dropblock_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = self.cbam(out)\n",
    "\n",
    "        if self.training and self.dropblock_prob > 0:\n",
    "            keep_prob = 1 - self.dropblock_prob\n",
    "            if torch.rand(1).item() > keep_prob:\n",
    "                return identity\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=11, dropblock_prob=0.1):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNet-50 結構：3-4-6-3 個 Bottleneck 塊\n",
    "        self.layer1 = self._make_layer(256, 3, stride=1, dropblock_prob=dropblock_prob)\n",
    "        self.layer2 = self._make_layer(512, 4, stride=2, dropblock_prob=dropblock_prob)\n",
    "        self.layer3 = self._make_layer(1024, 6, stride=2, dropblock_prob=dropblock_prob)\n",
    "        self.layer4 = self._make_layer(2048, 3, stride=2, dropblock_prob=dropblock_prob)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride, dropblock_prob):\n",
    "        layers = []\n",
    "        layers.append(Bottleneck(self.in_channels, out_channels, stride, dropblock_prob))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(Bottleneck(self.in_channels, out_channels, dropblock_prob=dropblock_prob))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ResNet50(num_classes=11, dropblock_prob=0.1).to(device)\n",
    "    print(model)\n",
    "\n",
    "    dummy_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "    output = model(dummy_input)\n",
    "    print(f\"輸出維度: {output.shape}\")\n",
    "\n",
    "    def count_parameters(m):\n",
    "        return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    print(f\"模型參數量: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練階段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練設置\n",
    "NUM_EPOCHS = 300\n",
    "LEARNING_RATE = 0.0007\n",
    "PATIENCE = 100\n",
    "MODEL_SAVE_PATH = \"best_model.pth\"\n",
    "CHECKPOINT_DIR = \"ResNet50_checkpoints\"\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "# Focal Loss \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for images, labels in train_loader_tqdm:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    val_loader_tqdm = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_tqdm:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_loss = running_loss / total\n",
    "    val_acc = 100.0 * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device):\n",
    "    class_counts = [994, 429, 1500, 986, 848, 1325, 440, 280, 855, 1500, 709]\n",
    "    total_samples = sum(class_counts)\n",
    "    num_classes = len(class_counts)\n",
    "    class_weights = torch.tensor([total_samples / (num_classes * count) for count in class_counts], dtype=torch.float).to(device)\n",
    "    print(\"類別權重:\", class_weights)\n",
    "\n",
    "    criterion = FocalLoss(gamma=2.0, alpha=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
    "\n",
    "    warmup_epochs = 10\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS - warmup_epochs, eta_min=5e-6)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    val_acc_window = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = LEARNING_RATE * (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print(f\"Warmup LR: {lr:.6f}\")\n",
    "        else:\n",
    "            print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        val_acc_window.append(val_acc)\n",
    "        if len(val_acc_window) > 3:\n",
    "            val_acc_window.pop(0)\n",
    "        smoothed_val_acc = sum(val_acc_window) / len(val_acc_window)\n",
    "\n",
    "        if epoch >= warmup_epochs:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.2f}% (Smoothed: {smoothed_val_acc:.2f}%)\")\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"model_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"--> 模型保存至 {checkpoint_path}\")\n",
    "\n",
    "        if smoothed_val_acc > best_val_acc:\n",
    "            best_val_acc = smoothed_val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"--> 最佳模型保存 (Smoothed Val Acc: {best_val_acc:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"--> 早停於 Epoch {epoch+1}, 最佳 Smoothed Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, best_val_acc, best_epoch\n",
    "\n",
    "def plot_accuracies(train_accuracies, val_accuracies):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Train and Validation Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig('accuracy_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ResNet50(num_classes=11).to(device)\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, best_val_acc, best_epoch = train_model(\n",
    "        model, train_loader, val_loader, device\n",
    "    )\n",
    "    plot_accuracies(train_accuracies, val_accuracies)\n",
    "    print(f\"訓練完成！最佳 Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理階段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing (TTA):   0%|          | 0/53 [00:00<?, ?it/s]/home/chihhung/.conda/envs/test/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Inferencing (TTA):   4%|▍         | 2/53 [02:38<1:07:14, 79.10s/it, num_processed=256]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# 增強 TTA 變換函數\n",
    "def get_tta_variants(tensor_image, variant_set=\"full\"):\n",
    "    tta_images = []\n",
    "    weights = []\n",
    "\n",
    "    # 基本變換（高權重）\n",
    "    tta_images.append(tensor_image)  # 原始圖片\n",
    "    weights.append(1.0)\n",
    "    tta_images.append(TF.hflip(tensor_image))  # 水平翻轉\n",
    "    weights.append(1.0)\n",
    "\n",
    "    # 旋轉變換\n",
    "    rotations = [10, -10, 20, -20, 30, -30]\n",
    "    if variant_set != \"full\":\n",
    "        rotations = random.sample(rotations, 4)  # 隨機選擇 4 個旋轉角度\n",
    "    for angle in rotations:\n",
    "        tta_images.append(TF.rotate(tensor_image, angle))\n",
    "        weights.append(0.8)  # 旋轉變換權重稍低\n",
    "\n",
    "    # 縮放變換\n",
    "    scales = [0.8, 0.9, 1.1, 1.2]\n",
    "    if variant_set != \"full\":\n",
    "        scales = random.sample(scales, 2)  # 隨機選擇 2 個縮放比例\n",
    "    for scale in scales:\n",
    "        scaled = TF.resize(tensor_image, int(IMG_SIZE * scale))\n",
    "        scaled = TF.resize(scaled, IMG_SIZE)\n",
    "        tta_images.append(scaled)\n",
    "        weights.append(0.7)  # 縮放變換權重更低\n",
    "\n",
    "    # 亮度、對比度和飽和度變化\n",
    "    brightness_factors = [0.8, 1.2]\n",
    "    contrast_factors = [0.8, 1.2]\n",
    "    if variant_set != \"full\":\n",
    "        brightness_factors = random.sample(brightness_factors, 1)\n",
    "        contrast_factors = random.sample(contrast_factors, 1)\n",
    "    for bf in brightness_factors:\n",
    "        tta_images.append(TF.adjust_brightness(tensor_image, bf))\n",
    "        weights.append(0.6)\n",
    "    for cf in contrast_factors:\n",
    "        tta_images.append(TF.adjust_contrast(tensor_image, cf))\n",
    "        weights.append(0.6)\n",
    "\n",
    "    # 隨機裁剪\n",
    "    if variant_set == \"full\" or random.random() > 0.5:\n",
    "        tta_images.append(TF.center_crop(TF.pad(tensor_image, padding=10), IMG_SIZE))\n",
    "        weights.append(0.5)\n",
    "\n",
    "    return tta_images, weights\n",
    "\n",
    "# 增強 TTA 推理函數（包含權重平均、後處理和投票）\n",
    "def inference_tta(model, test_loader, device, num_votes=3):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    indices = []\n",
    "\n",
    "    test_loader_tqdm = tqdm(test_loader, desc=\"Inferencing (TTA)\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_images, img_paths in test_loader_tqdm:\n",
    "            batch_size = batch_images.size(0)\n",
    "            batch_preds = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                single_img = batch_images[i].to(device)\n",
    "\n",
    "                # 多次投票\n",
    "                vote_preds = []\n",
    "                for vote in range(num_votes):\n",
    "                    # 每次投票使用隨機子集變換\n",
    "                    tta_imgs, weights = get_tta_variants(single_img, variant_set=\"subset\")\n",
    "\n",
    "                    # 蒐集各 TTA 影像的 logits\n",
    "                    all_logits = []\n",
    "                    for tta_img in tta_imgs:\n",
    "                        tta_img_batch = tta_img.unsqueeze(0)\n",
    "                        outputs = model(tta_img_batch)\n",
    "                        all_logits.append(outputs)\n",
    "\n",
    "                    # 權重平均 logits\n",
    "                    all_logits = torch.stack(all_logits, dim=0)  # [num_variants, 1, num_classes]\n",
    "                    weights_tensor = torch.tensor(weights, device=device).view(-1, 1, 1)\n",
    "                    avg_logits = torch.sum(all_logits * weights_tensor, dim=0) / sum(weights)\n",
    "                    _, pred = torch.max(avg_logits, dim=1)\n",
    "                    vote_preds.append(pred.item())\n",
    "\n",
    "                # 多數投票\n",
    "                final_pred = Counter(vote_preds).most_common(1)[0][0]\n",
    "\n",
    "                # logits 後處理（針對類別 4 和 5、8 和 9 的混淆）\n",
    "                tta_imgs, weights = get_tta_variants(single_img, variant_set=\"full\")\n",
    "                all_logits = []\n",
    "                for tta_img in tta_imgs:\n",
    "                    tta_img_batch = tta_img.unsqueeze(0)\n",
    "                    outputs = model(tta_img_batch)\n",
    "                    all_logits.append(outputs)\n",
    "                all_logits = torch.stack(all_logits, dim=0)\n",
    "                weights_tensor = torch.tensor(weights, device=device).view(-1, 1, 1)\n",
    "                avg_logits = torch.sum(all_logits * weights_tensor, dim=0) / sum(weights)\n",
    "                probs = F.softmax(avg_logits, dim=1).squeeze(0)\n",
    "\n",
    "                # 後處理：如果類別 4 和 5 的機率接近，根據先驗調整\n",
    "                if final_pred in [4, 5]:\n",
    "                    prob_4, prob_5 = probs[4].item(), probs[5].item()\n",
    "                    if abs(prob_4 - prob_5) < 0.1:\n",
    "                        if prob_4 > prob_5 and 848 / (848 + 1325) > 0.5:\n",
    "                            final_pred = 4\n",
    "                        else:\n",
    "                            final_pred = 5\n",
    "\n",
    "                # 後處理：如果類別 8 和 9 的機率接近，根據先驗調整\n",
    "                if final_pred in [8, 9]:\n",
    "                    prob_8, prob_9 = probs[8].item(), probs[9].item()\n",
    "                    if abs(prob_8 - prob_9) < 0.1:\n",
    "                        if prob_8 > prob_9 and 855 / (855 + 1500) > 0.5:\n",
    "                            final_pred = 8\n",
    "                        else:\n",
    "                            final_pred = 9\n",
    "\n",
    "                batch_preds.append(final_pred)\n",
    "\n",
    "            for img_path, pred in zip(img_paths, batch_preds):\n",
    "                index = int(os.path.basename(img_path).split('.')[0])\n",
    "                indices.append(index)\n",
    "                predictions.append(pred)\n",
    "\n",
    "            test_loader_tqdm.set_postfix(num_processed=len(indices))\n",
    "\n",
    "    return indices, predictions\n",
    "\n",
    "# 保存推理結果\n",
    "def save_results(indices, predictions, output_file=\"SampleSubmission.csv\"):\n",
    "    results = list(zip(indices, predictions))\n",
    "    results.sort(key=lambda x: x[0])  \n",
    "    df = pd.DataFrame(results, columns=[\"Index\", \"Label\"])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"推理結果保存到 {output_file}\")\n",
    "\n",
    "\n",
    "def run_inference(model, test_loader, device):\n",
    "    model = model.to(device)\n",
    "    indices, predictions = inference_tta(model, test_loader, device, num_votes=3)\n",
    "    save_results(indices, predictions, output_file=\"SampleSubmission.csv\")\n",
    "    \n",
    "\n",
    "def load_model(model, model_path=\"EfficientNetB4_checkpoints/best_model.pth\", device=\"cuda\"):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()  # 設置為評估模式\n",
    "    return model\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomEfficientNetB4(num_classes=11, dropblock_prob=0.3)\n",
    "run_inference(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Xception、InceptionV3、EfficientNet、ResNet152V2 都是目前不錯的CNN架構"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我選擇 EfficientNet 來實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomEfficientNetB4(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU()\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): MBConv(\n",
      "      (expand): Identity()\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(48, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(3, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (expand): Identity()\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(24, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(128, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5-7): 3 x MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(56, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(224, 224, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=224, bias=False)\n",
      "        (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(224, 14, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(14, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(224, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(56, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=224, bias=False)\n",
      "        (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(224, 14, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(14, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(224, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9-11): 3 x MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(112, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448, bias=False)\n",
      "        (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(448, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(28, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(448, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "        (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(42, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13-17): 5 x MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(60, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=960, bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(60, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (19-25): 7 x MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
      "        (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1632, 102, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(102, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (26): MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
      "        (1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1632, 102, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(102, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (27): MBConv(\n",
      "      (expand): Sequential(\n",
      "        (0): Conv2d(448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(2688, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (depthwise): Sequential(\n",
      "        (0): Conv2d(2688, 2688, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2688, bias=False)\n",
      "        (1): BatchNorm2d(2688, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): SiLU()\n",
      "      )\n",
      "      (cbam): CBAM(\n",
      "        (channel_attention): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "          (fc1): Conv2d(2688, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (fc2): Conv2d(168, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (spatial_attention): SpatialAttention(\n",
      "          (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU()\n",
      "    (3): AdaptiveAvgPool2d(output_size=1)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "    (6): Linear(in_features=1792, out_features=11, bias=True)\n",
      "  )\n",
      ")\n",
      "輸出維度: torch.Size([2, 11])\n",
      "模型參數量: 18,257,083\n"
     ]
    }
   ],
   "source": [
    "# CBAM 模塊\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(F.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(F.relu(self.fc1(self.max_pool(x))))\n",
    "        return torch.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_attention(x)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "# MBConv 模塊\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, expansion=6, dropblock_prob=0.0):\n",
    "        super(MBConv, self).__init__()\n",
    "        mid_channels = in_channels * expansion\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.SiLU()\n",
    "        ) if expansion > 1 else nn.Identity()\n",
    "\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels, mid_channels, kernel_size, stride=stride, padding=kernel_size//2, groups=mid_channels, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.cbam = CBAM(mid_channels)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(mid_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.dropblock_prob = dropblock_prob\n",
    "        self.shortcut = stride == 1 and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.expand(x)\n",
    "        out = self.depthwise(out)\n",
    "        out = self.cbam(out)\n",
    "        out = self.project(out)\n",
    "\n",
    "        if self.shortcut and self.training and self.dropblock_prob > 0:\n",
    "            keep_prob = 1 - self.dropblock_prob\n",
    "            if torch.rand(1).item() < keep_prob:\n",
    "                out += identity\n",
    "            return out\n",
    "        elif self.shortcut:\n",
    "            return out + identity\n",
    "        return out\n",
    "\n",
    "class CustomEfficientNetB4(nn.Module):\n",
    "    def __init__(self, num_classes=11, dropblock_prob=0.1):\n",
    "        super(CustomEfficientNetB4, self).__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 48, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # 主結構\n",
    "        stages = [\n",
    "            (48, 24, 1, 3, 1, 2),   # (in_ch, out_ch, expansion, kernel_size, stride, blocks)\n",
    "            (24, 32, 4, 5, 2, 2),\n",
    "            (32, 56, 4, 5, 2, 4),\n",
    "            (56, 112, 4, 3, 2, 4),\n",
    "            (112, 160, 6, 5, 1, 6),\n",
    "            (160, 272, 6, 5, 2, 8),\n",
    "            (272, 448, 6, 3, 1, 2),\n",
    "        ]\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for in_ch, out_ch, exp, k, s, num_blocks in stages:\n",
    "            for i in range(num_blocks):\n",
    "                stride = s if i == 0 else 1\n",
    "                self.blocks.append(MBConv(in_ch if i == 0 else out_ch, out_ch, k, stride, exp, dropblock_prob))\n",
    "\n",
    "        # 頭部\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(448, 1792, 1, bias=False),\n",
    "            nn.BatchNorm2d(1792),\n",
    "            nn.SiLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1792, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CustomEfficientNetB4(num_classes=11, dropblock_prob=0.1).to(device)\n",
    "    print(model)\n",
    "\n",
    "    dummy_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "    output = model(dummy_input)\n",
    "    print(f\"輸出維度: {output.shape}\")\n",
    "\n",
    "    def count_parameters(m):\n",
    "        return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    print(f\"模型參數量: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練階段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練設置\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0005\n",
    "PATIENCE = 100\n",
    "MODEL_SAVE_PATH = \"best_model.pth\"\n",
    "CHECKPOINT_DIR = \"EfficientNetB4_checkpoints\"\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "# Focal Loss \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for images, labels in train_loader_tqdm:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    val_loader_tqdm = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_tqdm:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_loss = running_loss / total\n",
    "    val_acc = 100.0 * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device):\n",
    "    class_counts = [994, 429, 1500, 986, 848, 1325, 440, 280, 855, 1500, 709]\n",
    "    total_samples = sum(class_counts)\n",
    "    num_classes = len(class_counts)\n",
    "    class_weights = torch.tensor([total_samples / (num_classes * count) for count in class_counts], dtype=torch.float).to(device)\n",
    "    print(\"類別權重:\", class_weights)\n",
    "\n",
    "    criterion = FocalLoss(gamma=2.0, alpha=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
    "\n",
    "    warmup_epochs = 10\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS - warmup_epochs, eta_min=5e-6)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    val_acc_window = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = LEARNING_RATE * (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print(f\"Warmup LR: {lr:.6f}\")\n",
    "        else:\n",
    "            print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        val_acc_window.append(val_acc)\n",
    "        if len(val_acc_window) > 3:\n",
    "            val_acc_window.pop(0)\n",
    "        smoothed_val_acc = sum(val_acc_window) / len(val_acc_window)\n",
    "\n",
    "        if epoch >= warmup_epochs:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.2f}% (Smoothed: {smoothed_val_acc:.2f}%)\")\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"model_epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"--> 模型保存至 {checkpoint_path}\")\n",
    "\n",
    "        if smoothed_val_acc > best_val_acc:\n",
    "            best_val_acc = smoothed_val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"--> 最佳模型保存 (Smoothed Val Acc: {best_val_acc:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"早停於 Epoch {epoch+1}, 最佳 Smoothed Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, best_val_acc, best_epoch\n",
    "\n",
    "def plot_accuracies(train_accuracies, val_accuracies):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Train and Validation Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig('accuracy_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CustomEfficientNetB4(num_classes=11).to(device)\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, best_val_acc, best_epoch = train_model(\n",
    "        model, train_loader, val_loader, device\n",
    "    )\n",
    "    plot_accuracies(train_accuracies, val_accuracies)\n",
    "    print(f\"訓練完成！最佳 Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
